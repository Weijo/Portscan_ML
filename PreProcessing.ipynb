{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting drive\n",
        "For this to work, we made a dataset folder and placed the CSVs into that folder."
      ],
      "metadata": {
        "id": "Z-0UG7IreqiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A8julzUILnp",
        "outputId": "1a4ba66f-c551-45e4-9a51-974498becc32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "U92BdbpLe6EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv"
      ],
      "metadata": {
        "id": "pXIN5BsFI5E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing data from drive"
      ],
      "metadata": {
        "id": "hbBzSegue7Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "path = \"/content/drive/MyDrive/dataset/T1046_flow_raw_Gp11_Low Wai Qun.csv\"\n",
        "wq = pd.read_csv(path)\n",
        "\n",
        "path = \"/content/drive/MyDrive/dataset/T1190_flow_raw_Gp11_Tay Wei Jie.csv\"\n",
        "wj = pd.read_csv(path)\n",
        "\n",
        "path = \"/content/drive/MyDrive/dataset/T1048_flow_raw_Gp11_Quah Kian Yang.csv\"\n",
        "ky = pd.read_csv(path)\n",
        "\n",
        "path = \"/content/drive/MyDrive/dataset/T1053_flow_raw_Gp11_Koh Cheng Kiat.csv\"\n",
        "ck = pd.read_csv(path)\n",
        "\n",
        "path = \"/content/drive/MyDrive/dataset/T1548.001_flow_raw_Gp11_Ng Wei Liang.csv\"\n",
        "wl = pd.read_csv(path)\n",
        "\n",
        "path = \"/content/drive/MyDrive/dataset/T1210_flow_raw_Gp11_Lum Zheng Jie.csv\"\n",
        "zj = pd.read_csv(path)\n",
        "\n",
        "path = \"/content/drive/MyDrive/dataset/Packetbeat_Full_raw_Grp12_fixed.csv\"\n",
        "grp12 = pd.read_csv(path)\n",
        "\n",
        "path = \"/content/drive/MyDrive/dataset/T1595-T1570-T1020_Packetbeat_raw_Gp16_SimYewSiangMerrill-SimKaiChing-RachelWongSiHui-YeoHanJordan_fixed.csv\"\n",
        "grp16 = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "2DCUj69kJbGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fixing columns\n",
        "One of our dataset had their columns values placed one column apart. Below fixes it."
      ],
      "metadata": {
        "id": "QCU9imWte9q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fix Group 11 dataset for inconsistent columns"
      ],
      "metadata": {
        "id": "lKFYH0AG5Sad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix columns\n",
        "rows_to_shift = zj.loc[zj['type'].isna()].index\n",
        "zj.iloc[rows_to_shift,30:] = zj.iloc[rows_to_shift,30:].shift(1,axis=1)\n",
        "zj.loc[zj['host.id'].isna(), 'host.id'] = '-'"
      ],
      "metadata": {
        "id": "GdWyQ5alXq-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fix Group 12 and 16 dataset for inconsistent columns"
      ],
      "metadata": {
        "id": "5uXMfIUF5Zrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/dataset/' # Ensure that the dataset folder is in your Google Drive's root folder (can be a shortcut).\n",
        "\n",
        "def log_fix(log_filename):\n",
        "  final_data = []\n",
        "  # opening the CSV file\n",
        "  with open(dataset_path + log_filename, mode ='r')as file:\n",
        "    # reading the CSV file\n",
        "    csvFile = csv.reader(file)\n",
        "  \n",
        "    # displaying the contents of the CSV file\n",
        "    for n, lines in enumerate(csvFile):\n",
        "      if n == 0:\n",
        "        final_data.append(['@timestamp', '_id', '_index', '_score', 'agent.ephemeral_id', 'agent.hostname', 'agent.id', 'agent.name', 'agent.type', 'agent.version', 'bytes_in', 'bytes_out', 'destination.bytes', 'destination.ip', 'destination.mac', 'destination.packets', 'destination.port', 'ecs.version', 'event.action', 'event.category', 'event.dataset', 'event.duration', 'event.end', 'event.kind', 'event.start', 'event.type', 'flow.final', 'flow.id', 'host.architecture', 'host.containerized', 'host.hostname', 'host.id', 'host.ip', 'host.mac', 'host.name', 'host.os.codename', 'host.os.family', 'host.os.kernel', 'host.os.name', 'host.os.name.text', 'host.os.platform', 'host.os.type', 'host.os.version', 'network.bytes', 'network.community_id', 'network.packets', 'network.transport', 'network.type', 'source.bytes', 'source.ip', 'source.mac', 'source.packets', 'source.port', 'type'])\n",
        "      else:\n",
        "        if lines[53] == \"flow\":\n",
        "          if lines[47] == \"ipv4\":\n",
        "            final_data.append(lines[:54])\n",
        "        elif lines[64] == \"icmp\":\n",
        "          pass\n",
        "        elif lines[64] == \"flow\":\n",
        "          if lines[12] != '-' and lines[13] != '-':\n",
        "            for i in range(4):\n",
        "              lines.pop(45)\n",
        "            lines.pop(47)\n",
        "            for i in range(3):\n",
        "              lines.pop(50)\n",
        "            lines.pop(55)\n",
        "            lines.pop(43)\n",
        "            lines.pop(43)\n",
        "            final_data.append(lines[:54])\n",
        "          else:\n",
        "            if lines[54] == 'ipv4':\n",
        "              lines.pop(12)\n",
        "              lines.pop(12)\n",
        "              for i in range(4):\n",
        "                lines.pop(43)\n",
        "              lines.pop(45)\n",
        "              lines.pop(48)\n",
        "              lines.pop(48)\n",
        "              lines.pop(48)\n",
        "              lines.pop(53)\n",
        "              final_data.append(lines[:54])\n",
        "        elif lines[77] == \"flow\":\n",
        "          if lines[64] == \"ipv4\":\n",
        "            lines.pop(12)\n",
        "            lines.pop(12)\n",
        "            lines.pop(12)\n",
        "            lines.pop(18)\n",
        "            for i in range(11):\n",
        "              lines.pop(43)\n",
        "            lines.pop(45)\n",
        "            lines.pop(46)\n",
        "            for i in range(5):\n",
        "              lines.pop(48)\n",
        "            lines.pop(48)\n",
        "            lines.pop(53)\n",
        "            final_data.append(lines[:54])\n",
        "        elif lines[79] == \"flow\":\n",
        "          if lines[66] == \"ipv4\":\n",
        "            for i in range(3):\n",
        "              lines.pop(12)\n",
        "            lines.pop(18)\n",
        "            for i in range(13):\n",
        "              lines.pop(43)\n",
        "            lines.pop(45)\n",
        "            lines.pop(46)\n",
        "            for i in range(6):\n",
        "              lines.pop(48)\n",
        "            lines.pop(53)\n",
        "            final_data.append(lines[:54])\n",
        "        elif lines[80] == \"flow\":\n",
        "          if lines[66] == \"ipv4\":\n",
        "            for i in range(3):\n",
        "              lines.pop(12)\n",
        "            lines.pop(13)\n",
        "            for i in range(13):\n",
        "              lines.pop(43)\n",
        "            lines.pop(45)\n",
        "            lines.pop(46)\n",
        "            for i in range(7):\n",
        "              lines.pop(48)\n",
        "            lines.pop(53)\n",
        "            final_data.append(lines[:54])\n",
        "  with open(dataset_path + log_filename[:-4] + '_fixed.csv', 'w') as f: \n",
        "    write = csv.writer(f) \n",
        "    write.writerows(final_data) "
      ],
      "metadata": {
        "id": "qqVsqXEfoeo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_fix(\"Packetbeat_Full_raw_Grp12.csv\")\n",
        "log_fix(\"T1595-T1570-T1020_Packetbeat_raw_Gp16_SimYewSiangMerrill-SimKaiChing-RachelWongSiHui-YeoHanJordan.csv\")"
      ],
      "metadata": {
        "id": "OAxcd7Ojow39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labeling of data\n",
        "Label all port scan flows as attack, others as 0"
      ],
      "metadata": {
        "id": "dfSL8m-hfQW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_cleaning(df):\n",
        "  cols_to_convert = ['source.bytes', 'destination.bytes', 'bytes_in', 'bytes_out', 'network.bytes']\n",
        "\n",
        "  for i, row in df.iterrows():\n",
        "    for j in cols_to_convert:\n",
        "      row[j] = row[j].replace(\",\", \"\")\n",
        "      \n",
        "      if row[j][-2:] == 'MB':\n",
        "        df.at[i , j] = str(int(round(float(row[j][:-2]) * 1048576,0)))\n",
        "      elif row[j][-2:] == 'KB':\n",
        "        df.at[i , j] = str(int(round(float(row[j][:-2]) * 1024,0)))\n",
        "      elif row[j][-1:] == 'B':\n",
        "        df.at[i , j] = str(float(row[j][:-1]))"
      ],
      "metadata": {
        "id": "zpIX7moIZolw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label data\n",
        "wq['attack'] = 0\n",
        "wq.loc[(wq['source.ip'] == \"10.10.10.10\") & (wq['destination.ip'] == \"10.10.10.30\"), 'attack'] = 1\n",
        "\n",
        "wj['attack'] = 0\n",
        "ck['attack'] = 0\n",
        "ky['attack'] = 0\n",
        "wl['attack'] = 0\n",
        "\n",
        "zj['attack'] = 0\n",
        "zj_filter = (zj['source.ip'] == '10.10.10.10') & ((zj['destination.ip'] == '10.10.10.10') | (zj['destination.ip'] == '10.10.10.20') | (zj['destination.ip'] == '10.10.10.30')) & ((zj['destination.port'] != '3,306') & (zj['destination.port'] != '445'))\n",
        "zj.loc[zj_filter, 'attack'] = 1\n",
        "\n",
        "# Group 12\n",
        "data_cleaning(grp12)\n",
        "grp12['attack'] = 0\n",
        "grp12.loc[(grp12['source.port'] == 58145), 'attack'] = 1\n",
        "\n",
        "# Group 16\n",
        "data_cleaning(grp16)\n",
        "grp16['attack'] = 0\n",
        "\n",
        "for i, row in grp16.iterrows():\n",
        "  if float(row['network.bytes']) <= 1024 and row['source.ip'] == \"192.168.0.4\" and row['destination.ip'] == \"192.168.0.3\":\n",
        "    grp16.at[i , 'attack'] = '1'"
      ],
      "metadata": {
        "id": "lRTJvlG1JmX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining datasets into one"
      ],
      "metadata": {
        "id": "I9EBjFsRfXzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [wq, wj, ky, ck, wl, zj, grp12, grp16]\n",
        "\n",
        "dataset = pd.concat(frames)"
      ],
      "metadata": {
        "id": "4xr82-hGMZMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "5NiNarGOMhhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73267960-99f7-4412-dcf1-e8ae13255d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56641, 55)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning of data\n",
        "- Drop duplicate columns\n",
        "- Replace '-' with 0\n",
        "- Remove ','\n",
        "- Convert to float64\n",
        "- Filter out selected features"
      ],
      "metadata": {
        "id": "KUAJVS0wfa-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicates\n",
        "dataset.drop_duplicates(subset='_id', keep=\"first\")\n",
        "\n",
        "# Replace - with 0\n",
        "dataset.replace(\"-\", 0, inplace = True)\n",
        "\n",
        "# Convert to float64\n",
        "dataset.replace(',','', regex=True, inplace=True)\n",
        "\n",
        "dataset = dataset.astype({\n",
        "    'bytes_in': 'float64',\n",
        "    'bytes_out': 'float64',\n",
        "    'destination.bytes': 'float64', \n",
        "    'source.bytes': 'float64',\n",
        "    'network.bytes': 'float64',\n",
        "    'event.duration': 'float64',\n",
        "    'source.ip': 'str',\n",
        "    'destination.ip': 'str',\n",
        "    'source.port': 'float64',\n",
        "    'destination.port': 'float64',\n",
        "    'attack': 'float64'  \n",
        "})\n",
        "\n",
        "# Filter out features\n",
        "selected_columns = ['bytes_in', 'bytes_out', 'destination.bytes', 'source.bytes', 'network.bytes', 'event.duration', 'source.ip', 'destination.ip', 'source.port', 'destination.port', 'attack']\n",
        "filtered_dataset = dataset[selected_columns]"
      ],
      "metadata": {
        "id": "O6cJaDUwecXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output cleaned data"
      ],
      "metadata": {
        "id": "CNiwmxLNmO3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/dataset/all_data.csv\"\n",
        "filtered_dataset.to_csv(save_path, sep=',', index=False)"
      ],
      "metadata": {
        "id": "-DwWZO5odjAE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}